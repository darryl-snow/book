# Project Managers

Your role is the most important.

## Success Criteria
## Assemble the Team
## Make a Plan
### Roles & Responsibilities
### Performance Budget
## Daily Workflow

*Agile

Continuous Integration makes dev life easier. Happier, better developers and testers. More efficient cycle. Keeps people happy.

It's not about the work that you do, but the work you enable others to do.

Smaller teams can afford to keep goals small and finish them often. Then you can test faster and learn quicker. By working quickly you can make decisions quickly, things flow better, and other teams will be envious. Cut your chunks of work into smaller chunks. Keep going to the minimum. If you can see measured improvement then do the rest of the chunks. If not, you've only lost a day or two maximum. Communicating frequently about what you're releasing makes you feel truly agile. Releasing often not only makes the team look good internally, but improves the end result for users too.

### THINK HOW, NOT WHAT
Your product isn’t just a bunch of features - so stop focusing on them.

What matters is not what functionality your product has, but how it works. A sign-up process is not just a sign-up process, a checkout process is not just a checkout process, a button is not just a button, a rating system is not just a rating system. Think about how you can stand out by introducing something that everyone else might have but in a unique way - what you are selling at it’s core. Here are a few questions to answer:

What is needed for your product to function well?
How much can you take away from it without sacrificing the core product
Why will people be excited about it
Glasses have one primary purpose, to help you see. Everything on top of that - such as colors, shapes, sizes and logos - is a feature. Understand when you are working on your core product and when you are working on adding features. The benefits of thinking like this, is that it will help you establish a very clear and precise picture of what makes your product your product. This will help you understand the minimal work necessary to get a valuable product out the door, and why you are adding features when you are.

You will be surprised how much the “how” can help improving your product - and getting an MVP out the door.

### SHIPPED IS BETTER THAN PERFECT
The goal of sketching, wireframing and prototyping is delivering great product concepts, not great deliverables.

Nobody cares if your deliverables are great if they’re not finished. And few will care how perfect they are even if they are finished. The only people who may marvel at the deliverables are the UX designers, but they’re hopefully too busy to care about internal documentation. On the other hand, everyone wants or absolutely needs you to communicate the right level of details about what they need to do to make a great and feasible solution so they can ship better products, faster.

If you’ve sketched something on scrap paper that you’re confident is a solid solution that everyone can act on immediately, there’s no value in re-creating it in a wireframe or anywhere else. Just take a picture of it and share it where appropriate. In some cases, you may have to quickly replicate it for organizational purposes, but don’t make deliverables for the sake of it - you have better things to do.

When we launched Perch we were, in most cases gently, mocked for writing it in that language everyone loves to hate – PHP. We were quizzed as to why we were not using a trendy NoSQL database. Our technology decisions however, were entirely sensible. They were based on what we knew to be true – that our target customers would not have Python, Ruby on Rails or the latest database available on their cheap hosting. We couldn’t even rely on them to have up to date PHP.

When it comes to launching your own product, it is very tempting to use it as the chance to do all of those things you never get to do in your day job, or on client projects. At last YOU are the client, as a developer there is little more fun than getting to grips with a new technology, building things with the shiniest tools. However, will that serve your customers well? Will they even care, or worse will your latest and greatest languages, tools and techniques actually prevent them from using your product?

PHP isn’t fashionable, a NoSQL database could actually be a great fit for Perch, however if it isn’t available on our target customer’s hosting then there is no point considering it.

The people who will allow you to build a great business around your product are far more invested in your success, because they use your product every day. In our case customers have built their business around selling sites built using Perch. Often they are not developers, but designers and learning a new CMS takes a considerable investment of time. They put a great deal of faith in us, that we won’t turn Perch into something that they can’t use. They trust that we won’t sell out, or otherwise abandon them and we really value that trust.

* Concentrate on what the users need
* Address the whole experience from start to finish
* Make it simple and intuitive
* Build it using agile and iterative practices
* Choose an appropriate technology stack
* Deploy on a suitable hosting environment
* Automate testing and deployments
* Manage security and privacy through reusable processes
* Use data to drive decisions
* Default to open

Understand what people needWe must begin digital projects by exploring and pinpointing the needs of the people who will use the service, and the ways in which the service will fit into their lives. Whether the users are members of the public or government employees, policy makers must include real people in their design process from the very beginning. The needs of people — not constraints of government structures or silos — should drive technical and design decisions. We need to continually test the products we build with real people to keep us honest about what is important.checklist
* Early in the project, spend time with current and prospective users of the service
* Use a range of qualitative and quantitative user research methods to determine people’s goals, needs, and behaviors; be thoughtful about the time spent
* Test prototypes of possible solutions with real people, in the field if possible
* Document the findings about user goals, needs, behaviors, and preferences
* Share findings with the team and agency leadership
* Create a prioritized list of user stories, which are short descriptions of the goals the user is trying to accomplish
* As the digital service is being built, regularly test it with potential users to ensure it will meet peoples’ needs
key questions
    * What user needs will this service address?
    * Why does the user want or need this service?
    * Who are your key users?
    * Which people will have the most difficulty with your service?
    * What research methods were used?
    * What were the key findings from users’ current experience?
    * How were the findings documented? Where can future team members access the documentation?
    * How often are you testing with real people?


Address the whole experience, from start to finishWe must build digital services with an understanding of the range of ways a person might interact with our service, including the actions they take online, through a mobile application, on the phone, or in person. Every encounter should move the user closer towards the desired outcome, whether that encounter is online or offline.checklist
Understand the different points at which people will interact with the service – both online and in person
Identify pain points in the current way users interact with the service, and prioritize these according to user needs
Design the digital parts of the service so that they are integrated with the offline touch points people use to interact with the service
Develop metrics that will measure how well the service is meeting user needs, at each step of the service
key questions
What are the different ways (both online and offline) that people currently accomplish the task the digital service is designed to help with?
Where are user pain points in the current way people accomplish the task?
Where does this specific project fit into the larger way people currently obtain the service being offered?
What metrics will best indicate how well the service is working for its users?

Make it simple and intuitiveUsing a government service shouldn’t be stressful, confusing, or daunting — it’s our job to build services that are simple and intuitive enough that users succeed the first time, unaided.checklist
Create or use an existing, simple, and flexible design style guide for the service
Use the design style guide across related digital services
Provide users with clear information about where they are in the process as they use the service
Follow accessibility best practices to ensure all people can use the service
Provide users with a way to exit and return later to complete the process
Use language that is familiar to the user and is easy to understand
Use language and design consistently throughout the service, including in the online and offline (non-digital) touch points people use to interact with the service
key questions
What primary tasks are the user trying to accomplish?
What is the reading level of the language the service uses?
What languages is your service offered in?
If a user needs help while using the service how do they go about getting it?
How does the service’s design visually relate to other government services?
PLAY 4

Build the service using agile and iterative practicesWe should use an incremental, fast-paced style of software development to reduce the risk of failure by getting working software into users’ hands quickly, and by providing frequent opportunities for the delivery team members to adjust requirements and development plans based on watching people use prototypes and real software. A critical capability is being able to automatically test and deploy the service so that new features can be added often and easily put into production. Following agile methodologies is a proven best practice for building digital services, and will increase our ability to build services that effectively meet user needs.checklist
Ship a functioning “minimum viable product” (MVP) that solves a core user need addressed by the service as soon as possible, and not longer than three months from the beginning of any new digital project, using a “beta” or “test” period if needed
Run usability tests frequently to see how well the service works for users, and identify improvements that should be made
Ensure the individuals building the service are in close communication using techniques such as war rooms, daily standups, and team chat tools
Keep delivery teams small and focused; limit organizational layers that separate these teams from the business owners
Release features and improvements multiple times each month
Create a prioritized list of features and bugs, also known as the “feature backlog” and “bug backlog”
Use an “issue tracker” to catalog features and bugs
Use a source code version control system
Ensure entire team has access to the issue tracker and version control system
Use code reviews to ensure quality
key questions
How long did it take to ship the MVP? If it has not shipped yet, when will it?
How long does it take for a production deployment?
How long in days are the iterations/sprints?
Which source code version control system is being used?
What tool is being used to track bugs and issue tickets?
What tool is being used to manage the feature backlog?
How often do you review and reprioritize the items in your feature and bug backlog?
How do you collect user feedback during development and how is that feedback to improve the service?
At each stage of usability testing, what gaps were identified in addressing user needs?

Choose a modern technology stackThe technology decisions we make need to enable development teams to work efficiently and enable services to scale easily and cost-effectively. Our choices for hosting infrastructure, databases, software frameworks, programming languages and the rest of the technology stack should seek to avoid vendor lock-in and match what successful modern consumer and enterprise software companies would choose today. In particular, digital services teams should consider using open source, cloud based, and commodity solutions across the technology stack, as these solutions have seen widespread adoption and support by the most successful private-sector consumer and enterprise software technology companies.checklist
Choose software frameworks that are commonly used by private-sector companies creating similar services
To the extent practical, ensure that software can be deployed on a variety of commodity hardware types
Ensure that each project has easy to understand instructions for setting up a local development environment, and that team members can be quickly added or removed from projects
Consider open source software solutions at all layers of the stack
key questions
What is your development stack and why did you choose it?
What database(s) are you using and why did you choose them?
How long does it take for a new team member to set up a local development environment?
PLAY 9

Deploy in a flexible hosting environmentOur services should be deployed on flexible infrastructure, where resources can be provisioned in real time to meet spikes in user demand. Our digital services are crippled when we host them in data centers which market themselves as “cloud hosting” but require us to manage and maintain hardware directly. This outdated practice wastes time, weakens our disaster recovery plans, and results in significantly higher costs.checklist
Resources are provisioned on demand
Resources scale based on real-time user demand
Resources are provisioned through an API
Resources are available in multiple regions
We pay only for the resources we use
Static assets are served through a content delivery network
Application is hosted on commodity hardware
key questions
Where is your service hosted?
What hardware does your service use to run?
What is the demand / usage pattern for your service?
What happens to your service when it experiences a surge in traffic or load?
How much capacity is available in your hosting environment?
How long does it take you to provision a new resource such as an application server?
How have you designed your service to scale based on demand?
How are you paying for your hosting infrastructure — i.e., by the minute, hourly, daily, monthly, fixed?
Is your service hosted in multiple regions / availability zones / data centers?
In the event of a catastrophic disaster to a datacenter, how long will it take to have the service operational?
What would be the impact of a prolonged downtime window?
What data redundancy do you have built into the system, and what would be the impact of a catastrophic data loss?
How often do you need to contact a person from your hosting provider to get resources or to fix an issue?
PLAY 10

Automate testing and deploymentsToday, developers write automated scripts that can verify thousands of scenarios in minutes and then deploy updated code into production environments multiple times per day. They use automated performance tests which simulate surges in traffic to identify performance bottlenecks. While manual tests and quality assurance is still necessary, automated tests provide consistent and reliable protection against unintentional regressions, and make it possible for developers to confidently release frequent updates to the service.checklist
Create automated tests that verify all user-facing functionality
Create unit and integration tests to verify modules and components
Run tests automatically as part of the build process
Perform deployments automatically with deployment scripts, continuous delivery services, or similar techniques
Conduct load and performance tests at regular intervals, including before public launch
key questions
What percentage of the code base is covered by automated tests?
How long does it take to build, test, and deploy a typical bug fix?
How long does it take to build, test, and deploy a new feature into production?
How frequently are builds created?
What test tools are used?
What deployment automation or continuous integration tools are used?
What is the estimated maximum number of concurrent users who will want to use the system?
How many simultaneous users could the system handle, according to the most recent capacity test?
How does the service perform when you exceed the expected target usage volume? Does the service degrade gracefully or catastrophically?
What is your scaling strategy when demand increases suddenly?
PLAY 11

Manage security and privacy through reusable processesIt is critical that our digital services protect sensitive information and keep systems secure. This is typically a process of continuous review and improvement which should be built into the development and maintenance of the service. At the start of designing a new service or feature, the team lead should engage the appropriate privacy, security, and legal officer(s) to discuss the type of information collected, how it should be secured, and how it may be used and shared. The sustained engagement of a privacy specialist helps ensure that personal data is properly managed. In addition, a key process to building a secure service is comprehensively testing and certifying the components in each layer of the technology stack for security vulnerabilities, and then to re-use these same pre-certified components for multiple services.The following checklist provides a starting point, but teams should work closely with their privacy specialist and security engineer to meet the needs of the specific service.checklist
Contact the appropriate privacy or legal officer of the department or agency to determine whether a System of Records Notice (SORN), Privacy Impact Assessment, or other review should be conducted
Determine, in consultation with a records officer, what data is collected and why, how it is used or shared, how it is stored and secured, and how long it is kept
Determine, in consultation with a privacy specialist, whether and how users are notified about how personal information is collected and used, including whether a privacy policy is needed and where it should appear, and how users will be notified in the event of a security breach
Consider whether the user should be able to access, delete, or remove their information from the service
“Pre-certify” the hosting infrastructure used for the project using FedRAMP
Use deployment scripts to ensure configuration of production environment remains consistent and controllable
key questions
Does the service collect personal information from the user (whether government or public)? How is the user notified of this collection?
Does it collect more information than is needed to perform the requested task? Are there uses of the data that would not be expected by the average user?
How does a user contact a responsible person to seek access, correction, deletion, or removal of his or her personal information?
Will information stored in the system be shared with others?
How and how often is the service tested for security vulnerabilities?
How can someone from the public report a security issue?

Use data to drive decisionsAt all stages of a digital project, we should measure how well our service is working for our users. This includes measuring how well a system performs and how people are interacting with the system in real time. Our teams and agency leadership should carefully watch these metrics to proactively spot issues and identify which improvements should be prioritized. In addition to monitoring tools, a feedback mechanism should be in place for people to report issues directly.checklist
Monitor system-level resource utilization in real time
Monitor system performance, measuring response time, latency, throughput, and error rates in real-time
Ensure monitoring in place can measure median, 95th percentile and 98th percentile performance
Create automated alerts based on this monitoring
Track concurrent users in real time, and monitor user behaviors (in the aggregate) to determine how well the service is meeting user needs
Publish metrics internally
Publish metrics externally
Use an experimentation tool that supports multivariate testing in production
key questions
What are the key metrics for the service?
How have these key metrics performed over the life of the service?
What system monitoring tool(s) are in place?
What is the targeted average response time for your service? What percent of requests take more than 1 second, 2 seconds, 4 seconds, and 8 seconds?
What is the average response time and percentile breakdown (percent of requests taking more than 1s, 2s, 4s, and 8s) for your service’s top 10 transactions?
What is your service’s monthly uptime target?
What is your service’s monthly uptime percentage including scheduled maintenance? Excluding scheduled maintenance?
How does your team receive automated alarms when incidents occur?
What is the volume of each of your service’s top 10 transactions? What is the percentage of transactions started vs. completed?
What tool(s) are in place to measure user behavior?
What tool/technology is used for A/B testing?
How do you measure customer satisfaction?

Default to openWhen we collaborate in the open and publish our data publicly we can improve Government together. By building services more openly and publishing open data, we simplify the public’s access to government services and information, allow the public to easily provide fixes and contributions, and enable reuse by entrepreneurs, nonprofits, other agencies, and the public.checklist
Offer users a mechanism to report bugs and issues, and be responsive to these reports
Provide datasets to the public, in their entirety, through bulk downloads and APIs (application programming interfaces)
Ensure that data from the service is explicitly in the public domain, and that rights are waived globally via an international public domain dedication, such as the “Creative Commons Zero” waiver
Catalog data in the agency’s enterprise data inventory and add any public datasets to the agency’s public data listing
Ensure that we maintain the rights to all data developed by third parties in such a manner that is releasable and reusable at no cost to the public
Ensure that we maintain contractual rights to all custom software developed by third parties in such a manner that is publishable and reusable at no cost
When appropriate, create an API for third parties to interact with the service directly
When appropriate, publish source code of projects or components online
When appropriate, share your development process and progress publicly
key questions
How are you collecting user feedback for bugs and issues?
If there is an API, what capabilities does it provide? Who uses it? How is it documented?
If the codebase has not been released under an open source license, explain why.
What components are made available to the public as open source?
What datasets are made available to the public?



Monitor system-level resource utilization in real time
Monitor system performance, measuring response time, latency, throughput, and error rates in real-time
Ensure monitoring in place can measure median, 95th percentile and 98th percentile performance
Create automated alerts based on this monitoring
Track concurrent users in real time, and monitor user behaviors (in the aggregate) to determine how well the service is meeting user needs
Publish metrics internally
Publish metrics externally
Use an experimentation tool that supports multivariate testing in production
key questions
What are the key metrics for the service?
How have these key metrics performed over the life of the service?
What system monitoring tool(s) are in place?
What is the targeted average response time for your service? What percent of requests take more than 1 second, 2 seconds, 4 seconds, and 8 seconds?
What is the average response time and percentile breakdown (percent of requests taking more than 1s, 2s, 4s, and 8s) for your service’s top 10 transactions?
What is your service’s monthly uptime target?
What is your service’s monthly uptime percentage including scheduled maintenance? Excluding scheduled maintenance?
How does your team receive automated alarms when incidents occur?
What is the volume of each of your service’s top 10 transactions? What is the percentage of transactions started vs. completed?
What tool(s) are in place to measure user behavior?
What tool/technology is used for A/B testing?

A product owner has been identified
All stakeholders agree that the product owner has the authority to assign tasks and make decisions about features and technical implementation details
The product owner has a product management background with technical experience to assess alternatives and weigh tradeoffs
The product owner has a work plan that includes budget estimates and identification of funding sources
The product owner has a strong relationship with his or her contracting officer
key questions
Who is the product owner?
What organizational changes have been made to ensure the product owner has sufficient authority over and support for the project?
What does it take for the product owner to add or remove a feature from the service?
PLAY 7

What is a product manager? What do product managers do all day? Most importantly, why do companies need to hire them? Good questions.
The first confusion we have to clear up is what we mean by “product.” In the context of software development, a product is the website, application or online service that users interact with. Depending on the size of the company and its products, a product manager could be responsible for an entire system (such as a mobile app) or part of a system (such as the checkout flow on an e-commerce website across all devices).
This is confusing because, in most contexts, a product is a thing you sell to people. Particularly in e-commerce, product managers often get confused with category managers, which are the team that sources and merchandises the products sold on an e-commerce website. So, yes, “product” probably isn’t the best word for it. But it’s what we’ve got, and it’s the word we’ll refer to in exploring this role.
To define the role of a product manager, let’s start by looking at Marc Andreessen’s view of the only thing that matters in a startup:
The quality of a startup’s product can be defined as how impressive the product is to one customer or user who actually uses it: How easy is the product to use? How feature rich is it? How fast is it? How extensible is it? How polished is it? How many (or rather, how few) bugs does it have?
The size of a startup’s market is the the number, and growth rate, of those customers or users for that product.…
The only thing that matters is getting to product-market fit. Product-market fit means being in a good market with a product that can satisfy that market.
Even though Andreessen wrote this for startups, the importance of that last sentence about product-market fit holds truth for every organization — whether the organization is getting a new product to market or redesigning an existing experience or anything in between. It is a universal road map to success, and it is the core of what product managers are responsible for.
With that as the backdrop, my definition of the role of a product manager would be to achieve business success by meeting user needs through the continual planning and execution of digital product solutions.
This definition summarizes all of the things that a product manager needs to obsess over: the target market, the intricacies of the product, what the business needs in order to succeed, and how to measure that success. It also encapsulates the three things that a product manager should never lose sight of:
The ultimate measure of success is the health of the business and, therefore, the value that the product provides to users.
Everything starts with a solid understanding of the target market and its needs, so that the focus remains on the quality of the product experience.
A continual cycle of planning and execution is required to meet these market needs in a sustainable way.
So, how does this translate to what a product manager does every day? That question is way too big to answer here, but as an introduction, Marty Cagan has a great list of common tasks that product managers are responsible for in his ebook Behind Every Great Product (PDF). The tasks include:
identifying and assessing the validity and feasibility of product opportunities,
making sure the right product is delivered at the right time,
developing a product strategy and road map for development,
leading the team in executing the product’s road map,
evangelizing the product internally to the executive team and colleagues,
representing customers through the product development process.
But before a product manager is able to do these things, a couple of awkward questions have to be asked. First, do companies really need product managers? And, if we can agree on that, what are the characteristics of a good one? Also, where does this role fit in an organization’s structure? Let’s explore these questions.
Why Companies Need Product Managers
The role of product manager can be a hard sell for some companies. Common objections to it include:
“We have various people in the organization whose roles fulfill each of these functions.”
“I don’t see how the role would make us more money.”
“Product managers would just slow us down.”
“I don’t want to relinquish control of the product to someone else.” (OK, this one is not usually said out loud.)
These appear to be valid concerns, but only if the role is not well understood — or if the organization has bad product managers who perpetuate these perceptions.
The truth is that, to be effective, the role of a manager for a particular product or area must not be filled by multiple people. It is essential for the product manager to see the whole picture — the strategic vision as well as the details of implementation — in order to make good decisions about the product. If knowledge of different parts of the process resides in the heads of different people, then no one will have that holistic view, and all value will be drained of the role.
Let’s look at two major benefits that product managers bring.
PRODUCT MANAGERS ENSURE A MARKET-DRIVEN APPROACH
The key argument in favor of product managers is that they help companies to be driven by the needs and goals of the target market, not the forces of technology or fads. As Barbara Nelson puts it in “Who Needs Product Management?”:
It is vastly easier to identify market problems and solve them with technology than it is to find buyers for your existing technology.
If done right, a market-driven focus results in long-term, sustainable, profitable business, because the company will remain focused on solving market problems, as opposed to looking for things to do with the latest technologies. A market-driven focus is important because companies that have one are proven to be more profitable than those driven by other factors (31% more profitable, according to George S. Day and Prakash Nedungadi).
This doesn’t mean focusing on incremental change to the exclusion of product innovation. Identifying market problems is about not only finding existing issues to improve (for example, “60% of users drop off on this page, so let’s fix that”), but also about creating new products to satisfy unmet needs (“Cell phones suck — let’s make a better one”).
PRODUCT MANAGERS IMPROVE TIME-TO-EVERYTHING
The second major benefit of product managers is that they reduce the time an organization takes to reach its goals. A well-defined and appropriate product development process run by effective managers will improve both the time-to-market as well as the time-to-revenue.
The reason for the faster turnaround time is that a product manager is responsible for figuring out what’s worth building and what’s not. This means less time spent on the spaghetti approach to product development (throwing things against the wall to see what sticks) and more time spent on building products that have been validated in the market. This approach also sharpens the organization’s focus, enabling the organization to dedicate more people to products that are likely to succeed, instead of spreading people too thin on projects that no one is sure will have a product-market fit.



7 Principles of Rich Web Applications

November 4, 2014 – 127504 views
This is a writeup based on a presentation I gave at BrazilJS in August 2014. It builds on some of the ideas I’ve been blogging about recently related mostly to UX and performance.

I want to introduce 7 actionable principles for websites that want to make use of JavaScript to control their UI. They are the result of my experience as a web developer, but also as a long-time user of the WWW.

JavaScript has undeniably become an indispensable tool for frontend developers. Its usage is now expanding into other areas like servers and microcontrollers. It’s the language of choice for introducing computer science concepts by prestigious universities.

Yet a lot of questions on its precise role and usage on the web remain a mystery, even to many framework and library authors.

Should JavaScript be used to replace browser functions like history, navigation and page rendering?
Is the backend dying? Should I render HTML at all?
Are Single Page Applications (SPAs) the future?
Is JS supposed to augment pages for websites, but render pages in web apps?
Should techniques like PJAX or TurboLinks be used?
What’s the precise distinction between a website and a web application? Should there be one at all?
What follows is my attempt to answer these. My approach is to examine the usage of JavaScript exclusively from the lens of user experience (UX). In particular, I put a strong focus on the idea of minimizing the time it takes the user to get the data they are interested in. Starting with networking fundamentals all the way to predicting the future.

Server rendered pages are not optional
Act immediately on user input
React to data changes
Control the data exchange with the server
Don’t break history, enhance it
Push code updates
Predict behavior
 1. Server rendered pages are not optional

tl;DR: Server rendering is not about SEO, it’s about performance. Consider the additional roundtrips to get scripts, styles, and subsequent API requests. In the future, considering HTTP 2.0 PUSH of resources.
The first thing I’m compelled to point out is a fairly common false dichotomy. That of “server-rendered apps vs single-page apps”. If we want to optimize for the best possible user experience and performance, giving up one or the other is never a good idea.

The reasons are fairly straightforward. The medium by which pages are transmitted, the internet, has a theoretical speed limit. This has been memorably illustrated by the famous essay/rant “It’s the latency, stupid” by Stuart Cheshire:

The distance from Stanford to Boston is 4320km.
The speed of light in vacuum is 300 x 10^6 m/s.
The speed of light in fibre is roughly 66% of the speed of light in vacuum.
The speed of light in fibre is 300 x 10^6 m/s * 0.66 = 200 x 10^6 m/s.
The one-way delay to Boston is 4320 km / 200 x 10^6 m/s = 21.6ms.
The round-trip time to Boston and back is 43.2ms.
The current ping time from Stanford to Boston over today’s Internet is about 85ms (…)
So: the hardware of the Internet can currently achieve within a factor of two of the speed of light.

The cited 85ms round-trip time between Boston and Stanford will certainly improve over time, and your own experiments right now might already show it. But it’s important to note that there’s a theoretical minimum of about 50ms between the two coasts.

The bandwidth capacity of your users’ connections might improve noticeably, as it steadily has, but the latency needle won’t move much at all. This means that minimizing the number of roundtrips you make to display information on page is essential to great user experience and responsiveness.

This becomes particularly relevant to point out considering the rise of JavaScript-driven applications that usually consist of no markup other than <script> and <link> tags beside an empty <body>. This class of application has received the name of “Single Page Applications” or “SPA”. As the name implies, there’s only one page the server consistently returns, and all the rest is figured out by your client side code.

Consider the scenario where the user navigates to http://app.com/orders/ after following a link or typing in the URL. At the time your application receives and processes the request, it already has important information about what’s going to be shown on that page. It could, for example, pre-fetch the orders from the database and include them in the response. In the case of most SPAs, a blank page and a <script> tag is returned instead, and another roundtrip will be made to get the scripts contents. So that then another roundtrip can be made to get the data needed for rendering.

Analysis of the HTML sent by the server for every page of a SPA in the wild
Analysis of the HTML sent by the server for every page of a SPA in the wild

At this point many developers consciously accept this tradeoff because they make sure the extra network hops happen only once for their users by sending the proper cache headers in the script and stylesheet responses. The general consensus is that it’s an acceptable tradeoff because once the bundle is loaded, you can then handle most of the user interaction (like transitions to other pages) without requesting additional pages or scripts.

However, even in the presence of a cache, there’s a performance penalty when considering script parsing and evaluation time. “Is jQuery Too Big For Mobile?” describes how even for jQuery alone this could be in the order of hundreds of milliseconds for certain mobile browsers.

What’s worse, usually no feedback whatsoever is given to the user while the scripts are loading. This results in a blank page displaying and then a sudden transition to a fully loaded page.

Most importantly, we usually forget that the current prevailing transport of internet data (TCP) starts slowly. This pretty much guarantees that most script bundles won’t be fetched in one roundtrip, making the situation described above even worse.

A TCP connection starts with an initial roundtrip for the handshake. If you’re using SSL, which happens to be important for safe script delivery, an additional two roundtrips are used (only one if the client is resuming a session). Only then can the server start sending data, but as it turns out, it does so slowly and incrementally.

A congestion control mechanism called slow start is built into the TCP protocol to send the data in a growing number of segments. This has two serious implications for SPAs:

Large scripts take a lot longer to download than it seems. As explained in the book “High Performance Browser Networking” by Ilya Grigorik, it takes “four roundtrips (…) and hundreds of milliseconds of latency, to reach 64 KB of throughput between the client and server”. In this example, considering a great internet connection between London and New York, it takes 225ms before TCP is able to reach the maximum packet size.
Since this rule applies also for the initial page download, it makes the initial content that comes rendered with the page all that much more important. As Paul Irish concludes in his presentation “Delivering the Goods”, the first 14kb are crucially important. This is a helpful illustration of the amount of data the server can send in each round-trip over time:

How many KB a server can send for each phase of the connection by segments.
How many KB a server can send for each phase of the connection by segments

Websites that deliver content (even if it’s only the basic layout without the data) within this window will seem extremely responsive. In fact, to many authors of fast server-side applications JavaScript is deemed unneeded or as something to be used sparingly. This bias is further strengthened if the app has a fast backend and data sources and its servers located near users (CDN).

The role of the server in assisting and speeding up content presentation is certainly application-specific. The solution is not always as straightforward as “render the entire page on the server”.

In some cases, parts of the page that are not essential to what the user is likely after are better left out of the initial response and fetched later by the client. Some applications, for example, opt to render the “shell” of the page to respond immediately. Then they fetch different portions of the page in parallel. This allows for great responsiveness even in a situation with slow legacy backend services. For some pages, pre-rendering the content that’s “above the fold” is also a viable option.

Making a qualitative assessment of scripts and styles based on the information the server has about the the session, the user and the URL is absolutely crucial. The scripts that deal with sorting orders will obviously be more important to /orders than the logic to deal with the settings page. Maybe less intuitively, one could also make a distinction between “structural CSS” and the “skin/theme CSS”. The former might be required by the JavaScript code, so it should block, but the latter could be loaded asynchronously.

A neat example of a SPA that does not incur in extra roundtrip penalties is a proof-of-concept clone of StackOverflow in 4096 bytes (which can theoretically be delivered on the first post-handshake roundtrip of a TCP connection!). It manages to pull this off at the expense of cacheability, by inlining all the assets within the response. With SPDY or HTTP/2 server push, it should be theoretically possible to deliver client code that’s cacheable in a single hop. For the time being, rendering part or all of the page on the server is the most common solution to avoiding extra roundtrips.

Proof-of-concept SPA with inlined CSS and JS<br />
that doesn’t incur in extra roundtrips
Proof-of-concept SPA with inlined CSS and JS that doesn’t incur in extra roundtrips

A flexible enough system that can share rendering code between browser and server and provides tools for progressively loading scripts and styles will probably eliminate the colloquial distinction between websites and webapps. Both are reigned by the same UX principles. A blog and a CRM are fundamentally not that different. They have URLs, navigation, they show data to the user. Even a spreadsheet application, which traditionally relies a lot more on client side functionality, first needs to show the user the data he’s interested in modifying. And doing so in the least number of network roundtrips is paramount.

In my view, the major tradeoffs in performance seen in many widely deployed systems these days have to do with the progressive accumulation of complexity in the stack. Technologies like JavaScript and CSS were added over time. Their popularity increased over time as well. Only now can we appreciate the impact of the different ways they’ve been applied. Some of this is addressed by improving protocols (as shown by the ongoing enhancements seen in SPDY and QUIC), but the application layer is where most of the benefits will come from.

It’s helpful to refer to some of the initial discussions around the design of the initial WWW and HTML to understand this. In particular, this mailing list thread from 1997 proposing the addition of the <img> tag to HTML. Marc Andreessen re-iterates the importance of serving information fast:

“If a document has to be pieced together on the fly, it could get arbitrarily complex, and even if that were limited, we’d certainly start experiencing major hits on performance for documents structured in this way. This essentially throws the single-hop principle of WWW out the door (well, IMG does that too, but for a very specific reason and in a very limited sense) — are we sure we want to do that?”

 2. Act immediately on user input

tl;DR: JavaScript allows us to mask network latency altogether. Applying this as a design principle should even remove most spinners or “loading” messages from your applications. PJAX or TurboLinks miss out on opportunities to improve the perception of speed.
The first principle builds heavily on the idea of minimizing latency as the user interacts with your website.

That said, despite how much effort you invest into minimizing the back-and-forth between server and client, there’s a few things beyond your control. A theoretical lower bound given by the distance between your user and your server being the unescapable one.

Poor or unpredictable network quality being the other significant one. If the network connection is not great, packet re-transmission will occur. What you would expect to result in a couple roundtrips could end up taking several.

And in this lies JavaScript’s greatest strength towards improving UX. With client-side code driving user interaction, we are now able to mask latency. We can create the perception of speed. We can artificially approach zero latency.

Let’s consider the basic HTML web again for a second. Documents connected together through hyperlinks, or <a> tags. When any of them are clicked, the browser will make a network request that’ll take unpredictably long, then get and process its response and finally transition to the new state.

JavaScript allows to act immediately and optimistically on user input. A click on a link or button can result in an immediate reaction without hitting the network. A famous example of this is Gmail (or Google Inbox), where archiving an email will happen immediately on the UI while the server request is sent and processed asynchronously.

In the case of a form, instead of waiting for some HTML as a response after its submission, we can act right after the user presses enter. Or even better, like Google Search does, we can respond to the user holding down a key:

Google adapts its layout as soon as you hold down a key
Google adapts its layout as soon as you hold down a key

That particular behavior is an example of what I call layout adaptation. The basic idea is that the first state of a page “knows” about the layout of the next state, so it can transition to it before there’s any data to populate the page with. It’s “optimistic” because there’s still a risk that the data never comes and an error should be displayed instead, but that’s obviously rare.

Google’s homepage is particularly relevant to this essay because its evolution illustrates the first two principles we’ve discussed very clearly.

First of all, analyzing the packet dump of the TCP connection to www.google.com reveals they make sure to send their entire homepage all at once after the request comes in. The whole exchange, including closing the connection, takes 64ms for me in San Francisco. This has likely been the case ever since the beginning.

In late 2004, Google pioneered the usage of JavaScript to provide inline as-you-type suggestions (curiously, as a 20% time project, like Gmail). This even became an inspiration for coining AJAX:

Take a look at Google Suggest. Watch the way the suggested terms update as you type, almost instantly … with no waiting for pages to reload. Google Suggest and Google Maps are two examples of a new approach to web applications that we at Adaptive Path have been calling Ajax

And in 2010 they introduced Instant Search, which puts JS front and center by skipping the page refresh altogether and transitioning to the “search results” layout as soon as you press a key as we saw above.

Another prominent example of layout adaptation is most likely in your pocket. Ever since the early days, iPhone OS would request app authors to provide a default.png image that would be rendered right away, while the actual app was loading.

iPhone OS enforced loading default.png before the application
iPhone OS enforced loading default.png before the application

In this case, the OS was compensating not necessarily for network latency, but CPU. This was crucial considering the constraints of the original hardware. There’s however a scenario where this technique breaks. That would be when the layout doesn’t match the stored image, as in the case of login screens. A thorough analysis of its implications was provided by Marco Arment in 2010.

Another form of input besides clicks and form submissions that’s greatly enhanced by JavaScript rendering is file input.

We can capture the user’s intent to upload through a variety of means: drag and drop, paste, file picker. Then, thanks to new HTML5 APIs we can display content as if it had been uploaded. An example of this in action is in the work we did with Cloudup uploads. Notice how the thumbnail is generated and rendered immediately:

The image gets rendered and fades in before the upload completes
The image gets rendered and fades in before the upload completes

In all of these cases, we’re enhancing the perception of speed. Thankfully, there’s plenty of evidence that this is a good idea. Consider the example of how increasing the walk to baggage claim reduced the number of complaints at the Houston Airport, without necessarily making baggage handling faster.

The application of this idea should have very profound implications on the UI of our applications. I contend that spinners or “loading indicators” should become a rarity, especially as we transition to applications with live data, discussed in the next section.

There’s situations where the illusion of immediacy could actually be detrimental to UX. Consider a payment form or a logout link. Acting optimistically on those, telling the user everything is done when it’s not, can result in a negative experience.

But even in those cases, the display of spinners or loading indicators should be deferred. They should only be rendered after the user no longer considers the response was immediate. According to the often-cited research by Nielsen:

The basic advice regarding response times has been about the same for thirty years Miller 1968; Card et al. 1991:
0.1 second is about the limit for having the user feel that the system is reacting instantaneously, meaning that no special feedback is necessary except to display the result.
1.0 second is about the limit for the user’s flow of thought to stay uninterrupted, even though the user will notice the delay. Normally, no special feedback is necessary during delays of more than 0.1 but less than 1.0 second, but the user does lose the feeling of operating directly on the data.
10 seconds is about the limit for keeping the user’s attention focused on the dialogue. For longer delays, users will want to perform other tasks while waiting for the computer to finish

Techniques like PJAX or TurboLinks unfortunately largely miss out on the opportunities described in this section. The client side code doesn’t “know” about the future representation of the page, until an entire roundtrip to the server occurs.

 3. React to data changes

tl;DR: When data changes on the server, let the clients know without asking. This is a form of performance improvement that frees the user from manual refresh actions (F5, pull to refresh). New challenges: (re)connection management, state reconciliation.
The third principle is that of reactivity of the UI with respect to data changes in the source, typically one or more database servers.

Serving an HTML snapshot of data that remains static until the user refreshes the page (traditional websites) or interacts with it (AJAX) is increasingly becoming obsolete.

Your UI should be self-updating.

This is crucially important in a world of an ever-increasing number of data points, in the form of watches, phones, tablets and wearable devices yet to be designed.

Consider the Facebook newsfeed at the time of its inception, when data was primarily entered through personal computers. Rendering it statically was not optimal, but it made sense if people were updating their profiles maybe once a day, if that.

We now live in a world where you can upload a photo, and have your peers like it or comment on it almost immediately. The need for realtime feedback is natural due to the highly concurrent usage of the application.

It would be wrong, however, to assume that the benefits of reactivity are limited to multi-user applications. Which is why I like to talk about  concurrent data points as opposed to users. Consider the common scenario of sharing a photo you have on your phone with your own laptop:

A single-user application can still benefit from reactivity
A single-user application can still benefit from reactivity

It’s useful to think of all the data exposed to the user as reactive. Session and login state synchronization is an example of applying this principle uniformly. If users of your application have multiple tabs open simultaneously, logging out of one will invalidate them all. This inevitably results in enhanced privacy and security, especially in situations where multiple people have access to the same device.

Each page reacts to the session and login state
Each page reacts to the session and login state

Once you set up the expectation that the information on the screen updates automatically, it’s important to consider a new need: state reconciliation.

When receiving ordered atomic data updates, it’s easy to forget that your application should be able to update appropriately even after long periods of disconnection. Consider the scenario of closing your laptop’s lid and reopening it days later. What how does your app behave then?

Example of what would occur if we disregard elapsed time upon reconnection
Example of what would occur if we disregard elapsed time upon reconnection

The ability for your application to reconcile states disjointed in time is also relevant to our first principle. If you opt to send data with the initial page load, you must consider the time the data is on the wire until your client-side scripts load. That time is essentially equivalent to a disconnection, and the initial connection by your scripts is a session resumption.

 4. Control the data exchange with the server

tl;DR: We can now fine-tune the data exchange with the server. Make sure to handle errors, retry on behalf of the user, sync data on the background and maintain offline caches.
When the WWW was conceived, data exchange between the client and server was limited to a few ways:

clicking a link would GET a new page and render the new page
submitting a form would POST or GET and render a new page
embedding an image or object would GET it asynchronously and render it
The simplicity of this model is attractive, and we certainly have a much higher learning curve today when it comes to understanding how data is sent and received.

The biggest limitations were around the second point. The inability to send data without necessarily triggering a new page load was not optimal from a performance standpoint. But most importantly, it completely broke the back button:


Possibly the most annoying artifact of the old web

The web as an application platform was thus inconceivable without JavaScript. AJAX constituted a leapfrog in terms of the user experience around user submission of information.

We now have a variety of APIs (XMLHttpRequest, WebSocket, EventSource to name a few) that give us fine-grained control of the data flow. In addition to the ability to send data the user inputs into a form, we now have some new opportunities to enhance UX.

One that’s specially relevant to our previous principle is the ability to display the connection state. If we set up the expectation that the data updates automatically, we ought to notify the user about being disconnected and ongoing reconnection attempts.

When detecting a disconnection, it’s useful to store data in memory (or even better, localStorage) so that it can be sent later. This is specially important in light of the introduction of ServiceWorker, which enables JavaScript web applications to run in the background. If your application is not open, you can still attempt to sync user data in the background.

Consider timeouts and errors when sending data and retry on behalf of the user. If a connection is re-established, attempt to send the data again. In the case of a persistent failure, communicate it to the user.

Certain errors should be handled carefully. For example, an unexpected 403 could mean the user’s session has been invalidated. In such cases, you have the opportunity to prompt the user to resume it by showing a login screen.

It’s also important to make sure the user doesn’t inadvertently interrupt the data flow. This can happen under two situations. The first and most obvious one is closing the browser or tab, which you can attempt to prevent with beforeunload handlers.


The beforeunload browser warning

The other (and less obvious) one is capturing page transitions before they happen, like clicking links that trigger a new page load. This gives you a chance to display your own modals.

 5. Don’t break history, enhance it

tl;DR: Without the browser managing URLs and history for us, new challenges emerge. Make sure not to break expectations related to scrolling. Keep your own caches for fast feedback.
Form submissions aside, if we were to design any modern web application with only hyperlinks, we’d end up with fully functional back/forward navigation.

Consider, for example, the typical “infinite pagination scenario”. The typical way it’s implemented involves capturing the click with JavaScript, requesting some data / HTML, injecting it. Making the history.pushState or replaceState call is an optional step, unfortunately not taken by many.

And this is why I use the word “break”. With the simpler model the web proposed initially, this situation was not in the picture. Every state transition relied on a URL change.

The flip side of this is that new opportunities emerge for enhancing history now that we can control it with JavaScript.

One such opportunity is what Daniel Pipius dubbed Fast Back:

Back should be quick; users don’t expect data to have changed much.

This is akin to considering the back button an application-level button and applying principle 2: act immediately on user input. The key is that you can now decide how to cache the previous page and render it instantly. You can then apply principle 3 and then inform the user of new data changes that happened to that page.

There’s still a few cases where you won’t be in control of the caching behavior. For example, if you render a page, then navigate to a third party website, and the user clicks back. Applications that render HTML on the server and then modify it on the client are at particular risk of this subtle bug:


Pressing back incorrectly loads the initial HTML from the pageload

Another way of breaking navigation is by ignoring scrolling memory. Once again, pages that don’t rely on JS and manual history management most likely won’t have an issue with this. But dynamic ones usually do. I tested the two most popular JavaScript-driven newsfeeds of the web: Twitter and Facebook. Both exhibited scrolling amnesia.


Infinite pagination is usually susceptible to scrolling amnesia

Finally, be aware of state changes that are relevant only while navigating history. Consider this example of toggling the display of comment subtrees.


The toggling of comments should be preserved when navigating history

If the page was re-rendered by following a link within the application, the expectation of the user might be that all comments appear uncollapsed. The state was volatile and only associated with the entry in the history stack.

 6. Push code updates

tl;DR: Pushing data without pushing code is insufficient. If your data updates automatically, so should your code. Avoid API errors and improve performance. Use stateless DOM for side-effect free repainting.
Making your application react to code changes is crucially important.

First of all, it reduces the surface for possible errors and increases reliability. If you make a breaking change to your backend APIs, then clients’ code must be updated. They might otherwise not be able to understand new data, or they may send data in an incompatible format.

Another equally important reason has to do with the implementation of principle #3. If your UI is self-updating, there’s little reason for users to trigger a page refresh.

Keep in mind that in a traditional website, a page refresh accomplishes two things: reload the data and reload the code. Setting up a mechanism to push data without one to push code is not enough, especially in a world where a single tab (session) might stay open for a very long time.

If a server push channel is in place, a notification can be emitted to clients when new code is available. In the absence of that, a version number can be appended as a header to outgoing HTTP requests. The server can then compare it to its latest known version, opt to handle request or not, and advice the client.

After this, some web applications opt to refresh the page on behalf of the user when deemed appropriate. For example, if the page is not visible and no form inputs are filled out.

A better approach is to perform hot code reloading. This means that there would be no need to perform a full page refresh. Instead, certain modules can be swapped on the fly and their code re-executed.

It’s certainly hard to make hot code reloading work for many existing codebases. It’s worth discussing then a type of architecture that elegantly separates behavior (code) from data (state). Such a separation would allow us to make a lot of different patches very efficient.

Consider for example a module in your application that sets up an event bus (e.g: socket.io). When events are received, the state of a certain component is populated and it renders to the DOM. Then you modify the behavior of that component, for example, so that it produces different DOM markup for existing and new state.

The ideal scenario is that we’re able to update the code on a per-module basis. It wouldn’t make sense to restart the socket connection, for example, if we can get away with just updating the modified component’s code. Our ideal architecture for hot-code pushing is thus modular.

But the next challenge is that modules should be able to be re-evaluated without introducing undesirable side effects. This is where an architecture like the one proposed by React comes particularly handy. If a component code is updated, its logic can be trivially re-executed and the DOM efficiently updates. An exploration of this concept by Dan Abramov can be found here.

In essence, the idea that you render to the DOM (or paint it) is what significantly helps with hot code swapping. If state was kept in the DOM, or event listeners where set up manually by your application, updating code would become a much more complicated task.

 7. Predict behavior

tl;DR: Negative latency.
A rich JavaScript application can have mechanisms in place for predicting the eventual user input.

The most common application of this idea is to preemptively request data from the server before an action is consummated. Starting to fetch data when you hover a hyperlink so that it’s ready when it’s clicked is a straightforward example.

A slightly more advanced method is to monitor mouse movement and analyze its trajectory to detect “collisions” with actionable elements like buttons. A jQuery example:

jQuery plugin that predicts the mouse trajectory
jQuery plugin that predicts the mouse trajectory

 Conclusion

The web remains one of the most versatile mediums for the transmission of information. As we continue to add more dynamism to our pages, we must ensure that we retain some of its great historical benefits while we incorporate new ones.

Pages interconnected by hyperlinks are a great building block for any type of application. Progressive loading of code, style and markup as the user navigates through them will ensure great performance without sacrificing interactivity.

New unique opportunities have been enabled by JavaScript that, once universally adopted, will ensure the best possible user experience for the broadest and freest platform in existence.

http://rauchg.com/2014/7-principles-of-rich-web-applications/


You Are Solving The Wrong Problem


There is some problem you are trying to solve. In your life, at work, in a design. You are probably solving the wrong problem. Paul MacCready, considered to be one of the best mechanical engineers of the 20th century, said it best: “The problem is we don’t understand the problem.”

Story time.

It’s 1959, a time of change. Disney releases their seminal film Sleeping Beauty, Fidel Castro becomes the premier of Cuba, and Eisenhower makes Hawaii an official state. That year, a British industry magnate by the name of Henry Kremer has a vision that leaves a haunting question: Can an airplane fly powered only by the pilot’s body power? Like Da Vinci, Kremer believed it was possible and decided to push his dream into reality. He offered the staggering sum of £50,000 for the first person to build a plane that could fly a figure eight around two markers one half-mile apart. Further, he offered £100,000 for the first person to fly across the channel. In modern US dollars, that’s the equivalent of $1.3 million and $2.5 million. It was the X-Prize of its day.



Paul MacCready holding a “Speed Ring”, a device he invented for competitive glider flying.

Thanks to Alan Kay for turning me on to this story.

A decade went by. Dozens of teams tried and failed to build an airplane that could meet the requirements. It looked impossible. Another decade threatened to go by before our hero, MacCready, decided to get involved. He looked at the problem, how the existing solutions failed, and how people iterated their airplanes. He came to the startling realization that people were solving the wrong problem. “The problem is,” he said, “that we don’t understand the problem.”

MacCready’s insight was that everyone working on solving human-powered flight would spend upwards of a year building an airplane on conjecture and theory without the grounding of empirical tests. Triumphantly, they’d complete their plane and wheel it out for a test flight. Minutes later, a years worth of work would smash into the ground. Even in successful flights, a couple hundred meters later the flight would end with the pilot physically exhausted. With that single new data point, the team would work for another year to rebuild, retest, relearn. Progress was slow for obvious reasons, but that was to be expected in pursuit of such a difficult vision. That’s just how it was.

The problem was the problem. Paul realized that what we needed to be solved was not, in fact, human powered flight. That was a red-herring. The problem was the process itself, and along with it the blind pursuit of a goal without a deeper understanding how to tackle deeply difficult challenges. He came up with a new problem that he set out to solve: how can you build a plane that could be rebuilt in hours not months. And he did. He built a plane with Mylar, aluminum tubing, and wire.



The first airplane didn’t work. It was too flimsy. But, because the problem he set out to solve was creating a plane he could fix in hours, he was able to quickly iterate. Sometimes he would fly three or four different planes in a single day. The rebuild, retest, relearn cycle went from months and years to hours and days.

18 years had passed since Henry Kremer opened his wallet for his vision. Nobody could turn that vision into an airplane. Paul MacCready got involved and changed the understanding of the problem to be solved. Half a year later later, MacCready’s Gossamer Condor flew 2,172 meters to win the prize. A bit over a year after that, the Gossamer Albatross flew across the channel.

What’s the take-away? When you are solving a difficult problem re-ask the problem so that your solution helps you learn faster. Find a faster way to fail, recover, and try again. If the problem you are trying to solve involves creating a magnum opus, you are solving the wrong problem.

http://www.azarask.in/blog/post/the-wrong-problem/


http://responsivedesign.is/articles/why-you-dont-need-device-specific-breakpoints

WHY YOU DON’T NEED DEVICE SPECIFIC BREAKPOINTS

BY JUSTIN AVERY, 28 OCT 2014 : OPINION

With the ever growing number of different mobile, tablet, laptops, monitors, televisions, watches — and whatever else will communicate information to you visually — it's finally time to put to rest those device specific breakpoints.

Recently I was fortunate to spend some time with Brad Frost. One of our conversations — most of which weren't anything to do with the industry — was around the need to upgrade to the latest round of devices (the iPhone 6 had recently been released).

I was joking about coming across my first iPad (2011 iPad 2) and how comically large it seemed after using the iPad Mini. With so many new devices that were now hitting the shelves and the fact that we're no longer wowed that there's a new bigger and skinnier phone has meant that for some the upgrade to the latest version isn't something that is as vital as it once was.

DESIRE TO UPGRADE

One of the reasons behind wanting to upgrade to the latest device was the need to start testing our own and our client websites to see what we needed to fix.

With the iPhone 6 new larger screen, and it's companions 6+ even bigger screen this would surely introduce a new set of design problems for all the sites we only launched since the iPhone 5 resize.

Surely this, along with the High DPI Screen was going to warrant us to head out and upgrade to the latest bit of hardware.

RESPONSIVE DESIGN AT ITS BEST

The fact is that we didn't need to do anything.

Nothing. Nada. Zip.

Over the past 4 years (that's right, the idea has been around for that long and it's not going anywhere) we've been moving towards a more responsive web. We have flexible grids, we have flexible images, and we use media queries to rearrange our layouts when the viewport dictates.

Over the 4 years we have slowly moved away from device specific breakpoints in favour of content specific breakpoints, i.e. adding a breakpoint when the content is no longer easy to consume.

With this fundamental shift in thinking we no longer have to fear a new device — a new iPhone width — because we've already fixed our sites to work everywhere.

Lets take a look at three of the most common issues with new widths and why they no longer matter.

FORM FACTOR

This is actually a good reason for testing. The size of the phone has increased and therefore the way we interface with the phone will change. Luke Wroblewski has written about the form factor and possible solutions to navigation.

This is nothing new though. There have been larger phones on the market since the Samsung Note was released so many of the issues faced will have already been overcome.

Apple have released a double tap on the home button to move the top of the screen closer for tapping elements, but this form factor is not available across all devices so not something that provides a well rounded solution.

Form factor doesn't matter because you've already taken these screen sizes into consideration with other devices.

HIGH DPI

When the retina display (Apples marketing name for high DPI screens) was released it caused a world wide panic. First there was 1.5 high dpi which meant you needed a 38px icon to render nice and crisp when displayed at 25px. Then we had the 2x icon sizes and we needed a 50px icon to render nice and crisp when displayed at 25px.

At some point we got bored or resizing icons and worked out that if we used Font Icons or SVG icons then it didn't matter how good the screen would become because the vector looks beautiful and crisp EVERYWHERE.

High DPI/Retina doesn't matter because you've already updated all your logo's and icons to SVG and that scales to work on any device.

NEW VIEWPORT DIMENSIONS

For a little while we lost site of what Responsive Design really meant — and I mean the underlying theory of Responsive Design where we respond our content to fit any viewport — and we began to use device widths to target particular layouts towards.

We knew that browser and device sniffing was frowned upon and thought that we could achieve the same approach using only media queries. This spurred the 320, 480, and 768px media queries that kept with the iPhone 3/4 and iPad viewport dimensions. When the iPhone 5 was released a handful of use updated the 480px query to 568px to accommodate the slightly larger screen. Strangely we never looked at Android phone dimensions (probably because there's far too many of them).

Fortunately at some point we all became aware we were doing it wrong and fell back to allowing the content dictate our breakpoints. This has the greatest benefit of all, the ability to still serve our sites content perfectly regardless of the device sizes released to market.

New device size don't matter because you've already based your media query breakpoints on the content.

THE FUTURE

This won't be the last iPhone to be released, and it certainly won't be the last smart phone released with a new dimension.

Fortunately it no longer matters the size of the devices that are coming in the future because we're already prepared for them with our flexible grids, flexible images, and content focussed breakpoints.

The picture proposal has meant that in the future image sizes are no longer an issue either, although we do need to become better at setting the sizes attribute.

The biggest issue we're going to have now will be producing content for smaller screens. Screens that fit on your wrist. This is a problem less for responsive design to solve, after all we can't reduce the content to less than a single column.

Instead the solution will be in the format we serve our content. Making it available not just in between <html>..</html> tags, but also in formats like <xml> <rss> and json.




04. The decline of the website

Have you noticed the gradual decline in the role of the website? Take for example going to see a movie. You know what you want to see, but you don’t know where it is showing.

In the past you would have visited each movie theatre website one at a time to see if they were showing the film you wanted. Each website was different, crafted by a busy team of web designers.

My betting is that is not how you look up movies anymore. The chances are you have a single app on your mobile that aggregates movie listings from many sources. Perhaps you even ask Siri or just Google it.

Web design trends for 2015
Who needs websites when you have Siri?
This creates a much better experience as users don't have to deal with different interfaces. Unfortunately it does start to undermine the role of the designer crafting these different sites.

I am sure it won't be long before you ask Siri and she tells you when and where your film is on. The whole thing done by voice command, no user interface at all.

Content is being set free from design. Instead we are sharing content via APIs between applications and sites. Sometimes business owners are choosing to put their content on Facebook, Yelp or Foursquare. They are abandoning the idea of having their own site. This is something that is particularly prevalent in China.




http://24ways.org/2014/is-agile-harder-for-agencies/

http://24ways.org/2014/making-sites-more-responsive-responsibly/

http://24ways.org/2014/responsive-enhancement/

http://davidwalsh.name/performance-cops-janitors

http://www.thesambarnes.com/dpm-interviews/learning-people-digital-project-manager-qa/

http://danielmall.com/articles/how-to-make-a-performance-budget/

http://alistapart.com/blog/post/responsive-web-design-second-ed

http://24ways.org/2014/developing-robust-deployment-procedures/

http://24ways.org/2014/what-it-takes-to-build-a-website/


http://thetomorrowlab.com/2015/01/why-developers-hate-being-interrupted/

Interruptions are to developers what kryptonite is to Superman—they kill productivity and there’s a significant recovery period.

There are two types of interruption: the planned meeting and the one where someone walks over to your desk to talk to you (or if you’re unlucky enough to have a desk phone it’s when the phone rings). The random interruption is akin to walking up to a someone building a lego tower, kicking it over and expecting them to continue from where they were the moment before you arrived. The planned meeting is a lot longer and kills productivity before, not just during and after. So, there are two types of problem that need addressed here.

WHAT HAPPENS WHEN A DEVELOPER IS INTERRUPTED?
A huge amount of what a developer is doing is in their head. As we write code we need to keep a mental model of how parts of the application that have already been written and are yet to be written interact with the part that we are writing at that moment. We need to have a solid picture of exactly how the code is working as a whole and maintain that picture. It’s not easy, it requires a lot of concentration and has to remain in place while we think of creative and efficient ways to solve the problem at hand.

Developers can appear very unproductive at times, sitting staring at the screen with our headphones on and very little in the way of keyboard clackety-tap. This however is when we are doing our thinking, when we are building up, adding to and rearranging the mental model of how our code will work. This is the biggest and hardest part of development.

Imagine how it feels to have that interrupted at random by a telephone call or somebody walking over to talk to you. It’s horrible.

This picture explains it very well.

The thing about this mental model is that it takes about an hour to build up from a cold start (first thing in the morning, after lunch or after a meeting). Thankfully that’s not the case after a five minute interruption from a boss or account manager. In that case it takes 10-15 minutes before a developer is back “in the zone” with a repaired mental model and ready to write some more code.

That’s still not great—we only need 4 or 5 small interruptions for an hour to be lost.

Scheduled meetings don’t bring the mental modal crashing down the way smaller interruptions do, but they have a significant impact. We know the meeting is taking place so we can stop where there’s a natural break point in our work and prepare for the meeting. Not such a huge problem there.

Where it becomes a problem is if the meeting is scheduled for an hour or so after lunch or arriving in the office. Remember that it takes about an hour to be in a position to write code after a longer break. Combine that with the fact that nobody can write much code in less than an hour, so anything that leaves a developer with less than two hours is probably going to mean it’s not worth the effort to get started only to stop and have to get started again afterwards.

There’s one more thing, a theory by developer come venture capitalist Paul Graham called Maker’s Schedule, Manager’s Schedule. In it Graham posits that managers are on a different schedule to makers and that is the root of the problem. Interruptions are the symptom.

It’s a good article and well worth reading. I feel it’s well summarised by two early paragraphs:

Most powerful people are on the manager’s schedule. It’s the schedule of command. But there’s another way of using time that’s common among people who make things, like programmers and writers. They generally prefer to use time in units of half a day at least. You can’t write or program well in units of an hour. That’s barely enough time to get started.

When you’re operating on the maker’s schedule, meetings are a disaster. A single meeting can blow a whole afternoon, by breaking it into two pieces each too small to do anything hard in. Plus you have to remember to go to the meeting. That’s no problem for someone on the manager’s schedule. There’s always something coming on the next hour; the only question is what. But when someone on the maker’s schedule has a meeting, they have to think about it.

The upshot of all of this is that developers often end up working late at night when there is no chance of an interruption and we know we can really get stuck into a project. Obviously this is not ideal. Developers are no more able to cope with a lack of sleep than anyone else, especially as we get past our twenties.

REDUCING INTERRUPTIONS
So what practical measures can we take to alleviate this?

Toby Osbourn has written a fantastic article on the subject of small interruptions and one of the things he highlights is the need for developers to turn off the interruptions we create ourselves like having our phone on a stand under our monitor, distracting us with every text, call, tweet and whatever other notifications it has turned on.

Desktop notifications need to go too. Close that email client and get rid of any browser tabs that aren’t related to the task in hand.

Here at The Tomorrow Lab we have a headphones rule. If a developer has headphones on or in both ears they cannot be disturbed. If only one ear has music then they can be disturbed about work. It’s been in place for a good while but for 2015 we decided to formalise it in the form of a poster on the wall, and you can download it to print if you like!

Another thing Osbourn suggests is moving somewhere else such as a coffee shop or working from home. Thankfully here at The Tomorrow Lab we can work from home frequently but moving out of the office is not something that would work for us over extended periods of time. There’s a lot of collaborative work with other members of the team on design, digital marketing, sales and with each other that we need to be in the office for.

The key thing for me though is communication with interrupters. They will never know how they affect our productivity unless we tell them. They spend all day doing their job and aren’t going to find out via telepathy that what they are doing is having a negative influence on how we do ours. Another system we are prototyping here is a timetable or quota of time when everybody has to stay away from developers unless there is a serious problem with a live site. We can’t impose this on account managers and directors so we need to work with them, effectively communicating why such a system needs to be in place and how it benefits the whole company.

That’s all fine for small interruptions, what about planned meetings? We can’t just take a laptop to a coffee shop to avoid meetings we’re supposed to be at. Meetings are a necessary evil. (Granted they are usually last twice as long as they need to which is a separate issue.)

To be honest we’re not sure the best way to tackle this one. We conducted an internal survey asking 4 questions:

What time(s) of day do you feel most productive?
What time(s) of day would you rather have meetings at?
What/who are the worst interruptions you have to deal with?
Anything else you want to say on the subject of being interrupted?
The initial feedback would suggest we’re not going to keep everyone happy when scheduling meetings but there are some “hotspot” times i.e. times you don’t dare touch when organsing a meeting. These hotspot times are in addition to the first couple of hours in the morning and after lunch. On top of that we aren’t robots working in The Tomorrow Lab so we each have different start times and lunch times.

I think the best we can hope for is that nobody ends up feeling like they’re always in meetings right when they are most likely to solve a hard problem.

HOW TO DEAL WITH INTERRUPTIONS WHEN THEY HAPPEN
We’re talking about short interruptions here, the unplanned type. They will happen—hopefully less than before—but they will happen, and the aim when dealing with them is to preserve the structural integrity of the mental model as much as possible.

First of all split up the main problem into a list of smaller tasks, so instead of thinking something like, “Now where was I with the progressively enhanced accessible modal dialogue”, it’s more like, “Now where was I with updating the aria-hidden attribute”.

Secondly learn how to answer the interrupter with something polite along the lines of, “I can’t look at it right now, send me an email to remind me and I’ll get back to you as soon as I can.” People will soon learn there’s not much point in interrupting you when they’ll just be going back to their desk to type out the same thing they were trying to tell you.

SUMMARY
Interruptions are a serious problem, they cause a huge amount in lost productivity, ergo money, and can make it very hard to get anything done. Working in a company where interruptions are guaranteed and nothing can be done about it is very demoralising. On the other hand, working in a company (*cough* like Tomorrow Lab *cough*) where you’re left to get on with your work is fantastic. It makes a developer feel like they can achieve something, it makes us look forward to going into work, it makes us feel fulfilled at the end of each day.

Developers don’t wear headphones because we enjoy music more than anyone else, it’s because headphones shut everything out and give us the mental space we need to build a very complicated model. (A lot of us view open plan offices as the worst invention of the 20th century!)

If you’re an interrupter and you get a terser response than you might have expected please don’t take it personally. We’re just aware of the model starting to loosen at the more precarious points and are getting frantic about the need to get back at work.

If you’re a developer and you’ve discovered some novel or interesting ways to reduce and/or deal with interruptions, please let us know in the comments.


http://sixrevisions.com/git/why-you-should-use-git/

Ever since version control systems such as Git have become widely-known and well-used, modern development processes have radically changed.

Here are some reasons why Git has the power to improve your development workflow.


Git Encourages Modularization
In even the smallest development projects, developers are often required to work on multiple components in parallel. Feature X, bug #102, a new UI for a sign-up form, etc.

Among many others, here are a couple of major issues with projects that aren’t version-controlled:

Project dependencies will be painful to manage. Team members must sort out which changes affect which components.
Unfinished and experimental code will mix-in with production-ready code. Without version control, there is a huge chance that an unstable piece of code gets deployed to a production environment.
Imagine these scenarios.

Your client tells you that they don’t want feature X anymore.

Or what if you find that feature Y — an experimental feature you have been working on — can’t be implemented?

How do you get code removed safely from your code base?

Using branches is the solution to these commonplace development problems. Although Git wasn’t the version control system (VCS) that introduced the concept of branching, it’s the first of its kind that makes it user-friendly.

Git Encourages Creativity and Experimentation
Git branching will improve your code-quality and productivity. It facilitates creativity and experimentation by removing your fear that the current version of the project will be affected while you are trying out ideas, giving you an environment where you can confidently explore and develop your project’s future features.

With Git, you can even create multiple branches that approach a given problem in different ways, test each of them with your master branch (the most current version of your project), and then choose the best option.

Git Allows You to Undo Your Mistakes
In Git, you can undo almost everything.

Being able to undo things gives your team the courage to try out ideas and concepts without the risk of breaking stuff, which in turn fosters a culture of innovation.

Git Makes Your Progress Clear
A commit in Git refers to the act of recording changes you have made in your source code.

When used properly, Git makes it easy to craft very granular commits. You can see what changes have occurred down to the microscopic level: Git will tell you what characters and lines in your source code have been altered. If you want to compare two versions of a file or the difference between two of your commits, you can issue Git’s diff command, which will highlight the differences between them.

A side note when using commits: As a good version control policy, I recommend that each commit should only contain changes that belong to a single topic.

For instance, if you’re fixing a specific bug — let’s call it bug #200 — that requires multiple source code changes across several files, all those changes should be under one commit in order for you to easily track changes to your project related to that bug. This way, you can document that "commit X fixed bug #200". A side benefit of this commit policy is, when you encounter another bug similar to bug #200 somewhere else in your project months from now, you can review how you resolved the first bug.

Mixing different topics in one commit makes it hard to see what things were added or resolved.

Also, multi-topic commits makes it more difficult to roll back to a previous version if you ever find the need to do so. Imagine a commit that contains changes for both feature A and feature B. If you find out later on that you introduced a severe security leak with feature B, you’ll have to roll back the code for both topics just to get rid of the problem.

You Can Work Offline
A lot of companies underestimate the benefits that can be had if their developers were to be able to work offline.

Being able to code on your laptop without having to connect to your company’s servers is not only about being able to work during a commute or while at home (which a lot of companies don’t permit due to security restrictions).

More importantly, being able to work offline makes you more fail-safe as a team: While with a VCS like Subversion or CVS, a developer can’t continue their work when a central server goes down, this is not an issue with Git.

In Git, developers can perform everything on their personal computer, making them independent of possible infrastructure downtimes.

Never Lose Data Ever Again
Losing data and not being able to recover it can break a dev project.

We’ve all heard or experienced our own set of horror stories about failed backups and servers. It’s good to know that when using Git, every developer working on a project has a full-fledged copy on their machine, including the project’s complete change history.

And if your team uses a remote source code repository such as GitHub, then the chances of losing your work in the event that your on-site backups fail is much smaller.

If your backups break down, losing data isn’t even a possibility when using Git: Just take any team member’s local repository and restore in minutes.

How to Use Git Today
Git has stepped out of being a technology for early-adopters. Today, more and more developers are using it to improve the quality of their code and workflow.
