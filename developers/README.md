# Developers

Your role is the most important.

## Setup
## Workflow
## Testing
## Coding
## More Testing

http://css-tricks.com/give-designers-tools-get-need/

Have your code work harder so your users won't have to.

If you're still doing the same thing as you did 2 years ago, or working in the same way, you're more than likely doing it wrong. You're doing the web wrong. The single most important skill for a web developer is the ability to learn new skills.

Best Practices:
READMEs and comments
Version control
Package Managers
Task Runners
Deployment Systems
CDN

http://blog.arvidandersson.se/2014/06/17/best-practices-in-modern-web-projects



http://www.sitepoint.com/good-developer/

How to be a Good Developer

 George Fekete
Published October 10, 2014

 Tweet
Subscribe
As a PHP developer, or any kind of developer as a matter of fact, you need to constantly improve yourself in this ever-changing industry; you need to learn and use new knowledge every day.

What successful developers have in common, is that they care about programming a lot, they are professionals treating good programming practices as a form of art.

In this article, you’ll learn about how to be a better developer by following the “etiquette” of programming and you’ll learn how to use this information to perhaps teach others to better themselves.

How to be a professional

Programmer's aid

Professionalism, regardless of the job you’re working on, always starts with you. Professionals first and foremost have strong personalities and characters.

As in any area of life, programming professionals are respected. Let’s see how you become one.

Don’t be egoistic

I’ve had the chance to work in large teams since I practice this craft and the most important team dynamic I learned early on is that team and collaboration goes hand in hand.

What you do most of the time in a team is learn from and teach each other, and the work environment should always embrace and reward sharing.

If you don’t want to share your work and knowledge, you’re arrogant and/or have a big ego, you won’t feel comfortable working in an environment like this.

Be responsible

Non-professionals don’t need to take responsibility for their own work. That’s left to the manager. They just get the job assigned to them and forget all about it when the clock hits 5 PM.

A professional programmer can’t accept this. How would you feel if your bug cost your company thousands of dollars?

This is a problem of which the solution also depends on management and how the company handles it. Every company should encourage developers to take responsibility of their actions and more importantly of the code they write.

If your bug slips onto the production server, do everything in your power to fix it as soon as possible, even if it takes all night long. This separates you from the nonprofessionals and gets you a higher paycheck.

Accept criticism

Software without bugs is impossible to write and we’re all victims of committing something stupid into the repository.

How we handle criticism says a lot about how we are looked at as developers.

Every criticism should be listened to and learned from, because that’s what makes you better at what you do, especially if you’re criticized by people who have way more experience than you do.

Have a strong work ethic

Being a professional is a non-stop job. Learning doesn’t last from 9 to 5.

Constantly learning, practicing and improving yourself is an investment in yourself and it’s your responsibility, not your employer’s.

This should also happen outside of work – you shouldn’t rob your employer’s time to read up on the latest SitePoint tutorials [Hey! Easy! ;) -Ed.].

There’s just not enough time, you say? Of course there is! You just have to think smart. If you want to take your career seriously, then focus on it seriously.

Get up early, leave a little bit late. Use those extra hours to your advantage without sacrificing your health and family.

Just half an hour before and after work means an extra five hours every week. That’s more than half an entire eight hour work day.

How to write good code

PHP Code

Read source code

Look at it this way: you can’t learn reading fast if you do not practice reading at all. The job of the developer is to write good code, but you can’t write good code if you don’t know what good code looks like.

Most developers blindly use third party libraries without touching the source code. This is okay to do, but to understand how that particular library can help, you need to dig in deeper and read its source code, the comments, run the tests (if it has any).

Reading code will help you quickly find other developers’ mistakes too and this helps a lot if you do code review or pair programming.

Learn new techniques

Always be open to learning new techniques and decide how they can help you be a better programmer.

Be open to new things all the time, don’t just dismiss the latest trends because you think they’ll pass. Everything is cyclical, but what’s constant is the knowledge you’re left with by opening your mind to it.

A good developer never stops learning, even with 15 – 20 years of practice behind him.

Slow down

Slowing down means to take a little bit more time on evaluating the problem you’re trying to solve. Being fast is not something you should strive for.

I’ve seen junior developers getting the task and delivering the code as fast as they could, resulting in buggy code, which took more time to fix than if they sat down and thought really hard of the right solution.

Senior developers are lazy and slow, and this is in everybody’s best interest, because a good programmer doesn’t want to do the job twice.

For a senior developer, writing the actual code takes up a third of his time spent on the task, the rest is thinking of a good solution for the problem.

Test your code

This won’t be a TDD or no TDD debate, but bear in mind that tests of any nature are very important for delivering quality code.

How do you know if something broke without testing it? Do you know what you were doing a couple of months ago on a particular feature’s code base?

From tests, you can learn how the code actually works. It’s like a guide for developers just as the table of contents of a book. Tests show you where to look and what to look for.

Writing tests for your code is important and hard at first, but it was proven to be beneficial in the long run countless times.

Know your toolset

Know what kind of tools you can use to help you fight the problem. Most of the tools, at the end of the day, come down to preference, but bear in mind that a good tool or library can help you out a lot.

Just think of how much time you spend in an editor, be it a full blown IDE or just a syntax highlighted text editor.

Also, you should decide whether it’s worth it to use a specific library for the job or not. Is it worth it to use a PHP framework? What are the pros and cons? Does using a clunky CMS for a project pay off?

These are the questions you should think of before even writing a single line of code.

How to stay on track

PHP Code

Fight burnout

Constantly pounding out code in a seemingly never ending cycle can be tiresome. Most developers who were in this business for long enough at some point in their career experienced burnout.

Burnout is associated with working long hours and what’s called the imposter syndrome, which means that a developer constantly thinks he’s not good enough and in order to be better he needs to work harder and to work more, while more doesn’t necessarily mean better.

The best medicine for this is to just step back, get out of that cycle and do other stuff, creative stuff. Take time off, even if it’s just a couple of days.

Another solution, increasingly popular in fighting burnout, is to find a team member with whom you can do pair programming. The social interaction with another human being is very effective.

Code maintenance

Staying on track also means keeping a clean code base. Not just for others, but for yourself, too. Code without tests or documentation is like Russian roulette.

What happens when you need to revisit some feature a couple of months down the road? You’ll spend more time figuring out what you were actually doing than on the task itself.

I’ve seen clients approaching developers to refactor their project countless times, because the previous team lost interest or couldn’t work on it anymore, and almost all the time the new team’s response was that it must be rewritten from scratch.

That happens because the previous team wasn’t capable of maintaining a clean, solid code base. This practice takes a lot of time; read the article called 18 Critical Oversights in Web Development which touches on how to keep code clean and other best practices.

On estimates

Estimates are a sensitive matter for many programmers and managers, and they shouldn’t be. I’m sure everybody heard of the case where managers ask developers how much time a task would take, and they expect clear answers, but the estimated task still takes up double the time that was initially estimated.

What most people fail to realize is that estimates are only guesses and not commitments. To be a better developer you should know that an estimate is never ever a commitment, because once you commit yourself to something, it means you’re responsible for delivering it.

Estimates never were and never will be commitments, this is the nature of an estimation. People are horrible at estimating time for a given task, and if your manager asks for this, you should tell him that you can’t commit yourself to something you’re not 100% sure of you can do on time.

You can, however, make a guess, but don’t make any promises.

How to be a master

PHP Code

Communication

It’s all about the communication. I’ve seen projects and companies fall apart because team members couldn’t communicate.

Keep communication simple and straightforward, cut out the middlemen between you and the receiver. Every “node” in your communication line creates almost exponential complications.

Enterprise suffers from this a lot – this is why it’s moving so slow, every decision has to go through a dozen people, and this is where agile teams shine.

Keeping communication simple and concise means you can move faster than others, you can understand your tasks more clearly and this gives you an advantage, so don’t be afraid to ask and to ask specific questions.

Collaborate

Besides being a good communicator you’ll also need to be a good collaborator, and let’s face it, programmers are not the most social people out there.

You need to collaborate not just with other developers, but also with your manager, and maybe directly with the client.

Collaboration also means knowing what’s at stake and to get the job done and to be a good team player.

If you find it hard to collaborate effectively with others, try out pair programming. The very essence of pair programming is collaboration.

See also this article on working with other people’s code.

The curse of knowledge

According to Wikipedia: “The curse of knowledge is a cognitive bias that leads better-informed parties to find it extremely difficult to think about problems from the perspective of lesser-informed parties.”

Basically, senior developers are having a hard time explaining problems so simple that junior developers can understand. This happens because they’re all very familiar with the problem and the techniques at hand to solve it, but when they try to explain it to others, they fail, because that explanation is just a summary of the knowledge in their head.

Simply put, when you know something, it’s very hard not knowing it. To fight this, you need to use specific language. Explain a problem in such detail that you find it funny even, but keep doing it, because your state of mind is not equal to the state of mind of the recipients.

Know your field

If you call yourself an expert in programming, then be an expert in programming. Know your field from top to bottom and don’t be afraid to say no as many times as you see fit.

To oversimplify this, being an expert is all about saying no to others, because that means you’re defending your truth, and having seniority among your peers, you’re probably right most of the time.

Knowing your field doesn’t necessarily mean you have a CS degree, it means you have a lot of experience and practice in what you do. You need to improve your skills not just in general programming, but in computer engineering and architecture.

Being an expert means you find the best possible programming design for a problem, writing code is the “side effect” of this.

Understand the business you’re in

Nobody can create good software without knowing the problems of the business and what they’re trying to solve with your code.

You need to be proactive and interested in the business, because that reflects onto your work. Without clear goals and specific problems the code will inadvertently be a mess, that’s how coding works.

You need to keep a tight leash on what features to implement and especially how, but for this the business value must be crystal clear.

If you feel that your expertise and the business’s goals do not align very well, then do yourself a favor and don’t accept the job. Value your time, because that’s priceless.

Code katas

To constantly improve yourself, first you must know at what level you are.

Code katas are exercises for programmers to improve their skills by practicing and finding better solutions for different problems.

You can try solving code katas at Project Euler, CodeKata or Topcoder.

Topcoder even offers prizes for finding the best solution to their programming challenges.

Conclusion

Programming is more a social skill than anything else. To be a good programmer, first you must work on your personality if you find yourself introverted. Then, master the programming principles.

You need to constantly improve yourself, to constantly learn, to be one step ahead of the game. To truly achieve professionalism you need to understand the business and the problem you’re trying to solve with your code.

Code is just a side product of the whole solution to the problem and it adds very little to the big picture. The ideas for solutions, the skills for collaboration and the mastery of the tools you need to use to solve a problem are the key to becoming a respected professional.


http://24ways.org/2014/naming-things/



Should JavaScript be used to replace browser functions like history, navigation and page rendering?
Is the backend dying? Should I render HTML at all?
Are Single Page Applications (SPAs) the future?
Is JS supposed to augment pages for websites, but render pages in web apps?
Should techniques like PJAX or TurboLinks be used?
What’s the precise distinction between a website and a web application? Should there be one at all?
What follows is my attempt to answer these. My approach is to examine the usage of JavaScript exclusively from the lens of user experience (UX). In particular, I put a strong focus on the idea of minimizing the time it takes the user to get the data they are interested in. Starting with networking fundamentals all the way to predicting the future.

Server rendered pages are not optional
Act immediately on user input
React to data changes
Control the data exchange with the server
Don’t break history, enhance it
Push code updates
Predict behavior
 1. Server rendered pages are not optional

tl;DR: Server rendering is not about SEO, it’s about performance. Consider the additional roundtrips to get scripts, styles, and subsequent API requests. In the future, considering HTTP 2.0 PUSH of resources.
The first thing I’m compelled to point out is a fairly common false dichotomy. That of “server-rendered apps vs single-page apps”. If we want to optimize for the best possible user experience and performance, giving up one or the other is never a good idea.

The reasons are fairly straightforward. The medium by which pages are transmitted, the internet, has a theoretical speed limit. This has been memorably illustrated by the famous essay/rant “It’s the latency, stupid” by Stuart Cheshire:

The distance from Stanford to Boston is 4320km.
The speed of light in vacuum is 300 x 10^6 m/s.
The speed of light in fibre is roughly 66% of the speed of light in vacuum.
The speed of light in fibre is 300 x 10^6 m/s * 0.66 = 200 x 10^6 m/s.
The one-way delay to Boston is 4320 km / 200 x 10^6 m/s = 21.6ms.
The round-trip time to Boston and back is 43.2ms.
The current ping time from Stanford to Boston over today’s Internet is about 85ms (…)
So: the hardware of the Internet can currently achieve within a factor of two of the speed of light.

The cited 85ms round-trip time between Boston and Stanford will certainly improve over time, and your own experiments right now might already show it. But it’s important to note that there’s a theoretical minimum of about 50ms between the two coasts.

The bandwidth capacity of your users’ connections might improve noticeably, as it steadily has, but the latency needle won’t move much at all. This means that minimizing the number of roundtrips you make to display information on page is essential to great user experience and responsiveness.

This becomes particularly relevant to point out considering the rise of JavaScript-driven applications that usually consist of no markup other than <script> and <link> tags beside an empty <body>. This class of application has received the name of “Single Page Applications” or “SPA”. As the name implies, there’s only one page the server consistently returns, and all the rest is figured out by your client side code.

Consider the scenario where the user navigates to http://app.com/orders/ after following a link or typing in the URL. At the time your application receives and processes the request, it already has important information about what’s going to be shown on that page. It could, for example, pre-fetch the orders from the database and include them in the response. In the case of most SPAs, a blank page and a <script> tag is returned instead, and another roundtrip will be made to get the scripts contents. So that then another roundtrip can be made to get the data needed for rendering.

Analysis of the HTML sent by the server for every page of a SPA in the wild
Analysis of the HTML sent by the server for every page of a SPA in the wild

At this point many developers consciously accept this tradeoff because they make sure the extra network hops happen only once for their users by sending the proper cache headers in the script and stylesheet responses. The general consensus is that it’s an acceptable tradeoff because once the bundle is loaded, you can then handle most of the user interaction (like transitions to other pages) without requesting additional pages or scripts.

However, even in the presence of a cache, there’s a performance penalty when considering script parsing and evaluation time. “Is jQuery Too Big For Mobile?” describes how even for jQuery alone this could be in the order of hundreds of milliseconds for certain mobile browsers.

What’s worse, usually no feedback whatsoever is given to the user while the scripts are loading. This results in a blank page displaying and then a sudden transition to a fully loaded page.

Most importantly, we usually forget that the current prevailing transport of internet data (TCP) starts slowly. This pretty much guarantees that most script bundles won’t be fetched in one roundtrip, making the situation described above even worse.

A TCP connection starts with an initial roundtrip for the handshake. If you’re using SSL, which happens to be important for safe script delivery, an additional two roundtrips are used (only one if the client is resuming a session). Only then can the server start sending data, but as it turns out, it does so slowly and incrementally.

A congestion control mechanism called slow start is built into the TCP protocol to send the data in a growing number of segments. This has two serious implications for SPAs:

Large scripts take a lot longer to download than it seems. As explained in the book “High Performance Browser Networking” by Ilya Grigorik, it takes “four roundtrips (…) and hundreds of milliseconds of latency, to reach 64 KB of throughput between the client and server”. In this example, considering a great internet connection between London and New York, it takes 225ms before TCP is able to reach the maximum packet size.
Since this rule applies also for the initial page download, it makes the initial content that comes rendered with the page all that much more important. As Paul Irish concludes in his presentation “Delivering the Goods”, the first 14kb are crucially important. This is a helpful illustration of the amount of data the server can send in each round-trip over time:

How many KB a server can send for each phase of the connection by segments.
How many KB a server can send for each phase of the connection by segments

Websites that deliver content (even if it’s only the basic layout without the data) within this window will seem extremely responsive. In fact, to many authors of fast server-side applications JavaScript is deemed unneeded or as something to be used sparingly. This bias is further strengthened if the app has a fast backend and data sources and its servers located near users (CDN).

The role of the server in assisting and speeding up content presentation is certainly application-specific. The solution is not always as straightforward as “render the entire page on the server”.

In some cases, parts of the page that are not essential to what the user is likely after are better left out of the initial response and fetched later by the client. Some applications, for example, opt to render the “shell” of the page to respond immediately. Then they fetch different portions of the page in parallel. This allows for great responsiveness even in a situation with slow legacy backend services. For some pages, pre-rendering the content that’s “above the fold” is also a viable option.

Making a qualitative assessment of scripts and styles based on the information the server has about the the session, the user and the URL is absolutely crucial. The scripts that deal with sorting orders will obviously be more important to /orders than the logic to deal with the settings page. Maybe less intuitively, one could also make a distinction between “structural CSS” and the “skin/theme CSS”. The former might be required by the JavaScript code, so it should block, but the latter could be loaded asynchronously.

A neat example of a SPA that does not incur in extra roundtrip penalties is a proof-of-concept clone of StackOverflow in 4096 bytes (which can theoretically be delivered on the first post-handshake roundtrip of a TCP connection!). It manages to pull this off at the expense of cacheability, by inlining all the assets within the response. With SPDY or HTTP/2 server push, it should be theoretically possible to deliver client code that’s cacheable in a single hop. For the time being, rendering part or all of the page on the server is the most common solution to avoiding extra roundtrips.

Proof-of-concept SPA with inlined CSS and JS<br />
that doesn’t incur in extra roundtrips
Proof-of-concept SPA with inlined CSS and JS that doesn’t incur in extra roundtrips

A flexible enough system that can share rendering code between browser and server and provides tools for progressively loading scripts and styles will probably eliminate the colloquial distinction between websites and webapps. Both are reigned by the same UX principles. A blog and a CRM are fundamentally not that different. They have URLs, navigation, they show data to the user. Even a spreadsheet application, which traditionally relies a lot more on client side functionality, first needs to show the user the data he’s interested in modifying. And doing so in the least number of network roundtrips is paramount.

In my view, the major tradeoffs in performance seen in many widely deployed systems these days have to do with the progressive accumulation of complexity in the stack. Technologies like JavaScript and CSS were added over time. Their popularity increased over time as well. Only now can we appreciate the impact of the different ways they’ve been applied. Some of this is addressed by improving protocols (as shown by the ongoing enhancements seen in SPDY and QUIC), but the application layer is where most of the benefits will come from.

It’s helpful to refer to some of the initial discussions around the design of the initial WWW and HTML to understand this. In particular, this mailing list thread from 1997 proposing the addition of the <img> tag to HTML. Marc Andreessen re-iterates the importance of serving information fast:

“If a document has to be pieced together on the fly, it could get arbitrarily complex, and even if that were limited, we’d certainly start experiencing major hits on performance for documents structured in this way. This essentially throws the single-hop principle of WWW out the door (well, IMG does that too, but for a very specific reason and in a very limited sense) — are we sure we want to do that?”

 2. Act immediately on user input

tl;DR: JavaScript allows us to mask network latency altogether. Applying this as a design principle should even remove most spinners or “loading” messages from your applications. PJAX or TurboLinks miss out on opportunities to improve the perception of speed.
The first principle builds heavily on the idea of minimizing latency as the user interacts with your website.

That said, despite how much effort you invest into minimizing the back-and-forth between server and client, there’s a few things beyond your control. A theoretical lower bound given by the distance between your user and your server being the unescapable one.

Poor or unpredictable network quality being the other significant one. If the network connection is not great, packet re-transmission will occur. What you would expect to result in a couple roundtrips could end up taking several.

And in this lies JavaScript’s greatest strength towards improving UX. With client-side code driving user interaction, we are now able to mask latency. We can create the perception of speed. We can artificially approach zero latency.

Let’s consider the basic HTML web again for a second. Documents connected together through hyperlinks, or <a> tags. When any of them are clicked, the browser will make a network request that’ll take unpredictably long, then get and process its response and finally transition to the new state.

JavaScript allows to act immediately and optimistically on user input. A click on a link or button can result in an immediate reaction without hitting the network. A famous example of this is Gmail (or Google Inbox), where archiving an email will happen immediately on the UI while the server request is sent and processed asynchronously.

In the case of a form, instead of waiting for some HTML as a response after its submission, we can act right after the user presses enter. Or even better, like Google Search does, we can respond to the user holding down a key:

Google adapts its layout as soon as you hold down a key
Google adapts its layout as soon as you hold down a key

That particular behavior is an example of what I call layout adaptation. The basic idea is that the first state of a page “knows” about the layout of the next state, so it can transition to it before there’s any data to populate the page with. It’s “optimistic” because there’s still a risk that the data never comes and an error should be displayed instead, but that’s obviously rare.

Google’s homepage is particularly relevant to this essay because its evolution illustrates the first two principles we’ve discussed very clearly.

First of all, analyzing the packet dump of the TCP connection to www.google.com reveals they make sure to send their entire homepage all at once after the request comes in. The whole exchange, including closing the connection, takes 64ms for me in San Francisco. This has likely been the case ever since the beginning.

In late 2004, Google pioneered the usage of JavaScript to provide inline as-you-type suggestions (curiously, as a 20% time project, like Gmail). This even became an inspiration for coining AJAX:

Take a look at Google Suggest. Watch the way the suggested terms update as you type, almost instantly … with no waiting for pages to reload. Google Suggest and Google Maps are two examples of a new approach to web applications that we at Adaptive Path have been calling Ajax

And in 2010 they introduced Instant Search, which puts JS front and center by skipping the page refresh altogether and transitioning to the “search results” layout as soon as you press a key as we saw above.

Another prominent example of layout adaptation is most likely in your pocket. Ever since the early days, iPhone OS would request app authors to provide a default.png image that would be rendered right away, while the actual app was loading.

iPhone OS enforced loading default.png before the application
iPhone OS enforced loading default.png before the application

In this case, the OS was compensating not necessarily for network latency, but CPU. This was crucial considering the constraints of the original hardware. There’s however a scenario where this technique breaks. That would be when the layout doesn’t match the stored image, as in the case of login screens. A thorough analysis of its implications was provided by Marco Arment in 2010.

Another form of input besides clicks and form submissions that’s greatly enhanced by JavaScript rendering is file input.

We can capture the user’s intent to upload through a variety of means: drag and drop, paste, file picker. Then, thanks to new HTML5 APIs we can display content as if it had been uploaded. An example of this in action is in the work we did with Cloudup uploads. Notice how the thumbnail is generated and rendered immediately:

The image gets rendered and fades in before the upload completes
The image gets rendered and fades in before the upload completes

In all of these cases, we’re enhancing the perception of speed. Thankfully, there’s plenty of evidence that this is a good idea. Consider the example of how increasing the walk to baggage claim reduced the number of complaints at the Houston Airport, without necessarily making baggage handling faster.

The application of this idea should have very profound implications on the UI of our applications. I contend that spinners or “loading indicators” should become a rarity, especially as we transition to applications with live data, discussed in the next section.

There’s situations where the illusion of immediacy could actually be detrimental to UX. Consider a payment form or a logout link. Acting optimistically on those, telling the user everything is done when it’s not, can result in a negative experience.

But even in those cases, the display of spinners or loading indicators should be deferred. They should only be rendered after the user no longer considers the response was immediate. According to the often-cited research by Nielsen:

The basic advice regarding response times has been about the same for thirty years Miller 1968; Card et al. 1991:
0.1 second is about the limit for having the user feel that the system is reacting instantaneously, meaning that no special feedback is necessary except to display the result.
1.0 second is about the limit for the user’s flow of thought to stay uninterrupted, even though the user will notice the delay. Normally, no special feedback is necessary during delays of more than 0.1 but less than 1.0 second, but the user does lose the feeling of operating directly on the data.
10 seconds is about the limit for keeping the user’s attention focused on the dialogue. For longer delays, users will want to perform other tasks while waiting for the computer to finish

Techniques like PJAX or TurboLinks unfortunately largely miss out on the opportunities described in this section. The client side code doesn’t “know” about the future representation of the page, until an entire roundtrip to the server occurs.

 3. React to data changes

tl;DR: When data changes on the server, let the clients know without asking. This is a form of performance improvement that frees the user from manual refresh actions (F5, pull to refresh). New challenges: (re)connection management, state reconciliation.
The third principle is that of reactivity of the UI with respect to data changes in the source, typically one or more database servers.

Serving an HTML snapshot of data that remains static until the user refreshes the page (traditional websites) or interacts with it (AJAX) is increasingly becoming obsolete.

Your UI should be self-updating.

This is crucially important in a world of an ever-increasing number of data points, in the form of watches, phones, tablets and wearable devices yet to be designed.

Consider the Facebook newsfeed at the time of its inception, when data was primarily entered through personal computers. Rendering it statically was not optimal, but it made sense if people were updating their profiles maybe once a day, if that.

We now live in a world where you can upload a photo, and have your peers like it or comment on it almost immediately. The need for realtime feedback is natural due to the highly concurrent usage of the application.

It would be wrong, however, to assume that the benefits of reactivity are limited to multi-user applications. Which is why I like to talk about  concurrent data points as opposed to users. Consider the common scenario of sharing a photo you have on your phone with your own laptop:

A single-user application can still benefit from reactivity
A single-user application can still benefit from reactivity

It’s useful to think of all the data exposed to the user as reactive. Session and login state synchronization is an example of applying this principle uniformly. If users of your application have multiple tabs open simultaneously, logging out of one will invalidate them all. This inevitably results in enhanced privacy and security, especially in situations where multiple people have access to the same device.

Each page reacts to the session and login state
Each page reacts to the session and login state

Once you set up the expectation that the information on the screen updates automatically, it’s important to consider a new need: state reconciliation.

When receiving ordered atomic data updates, it’s easy to forget that your application should be able to update appropriately even after long periods of disconnection. Consider the scenario of closing your laptop’s lid and reopening it days later. What how does your app behave then?

Example of what would occur if we disregard elapsed time upon reconnection
Example of what would occur if we disregard elapsed time upon reconnection

The ability for your application to reconcile states disjointed in time is also relevant to our first principle. If you opt to send data with the initial page load, you must consider the time the data is on the wire until your client-side scripts load. That time is essentially equivalent to a disconnection, and the initial connection by your scripts is a session resumption.

 4. Control the data exchange with the server

tl;DR: We can now fine-tune the data exchange with the server. Make sure to handle errors, retry on behalf of the user, sync data on the background and maintain offline caches.
When the WWW was conceived, data exchange between the client and server was limited to a few ways:

clicking a link would GET a new page and render the new page
submitting a form would POST or GET and render a new page
embedding an image or object would GET it asynchronously and render it
The simplicity of this model is attractive, and we certainly have a much higher learning curve today when it comes to understanding how data is sent and received.

The biggest limitations were around the second point. The inability to send data without necessarily triggering a new page load was not optimal from a performance standpoint. But most importantly, it completely broke the back button:


Possibly the most annoying artifact of the old web

The web as an application platform was thus inconceivable without JavaScript. AJAX constituted a leapfrog in terms of the user experience around user submission of information.

We now have a variety of APIs (XMLHttpRequest, WebSocket, EventSource to name a few) that give us fine-grained control of the data flow. In addition to the ability to send data the user inputs into a form, we now have some new opportunities to enhance UX.

One that’s specially relevant to our previous principle is the ability to display the connection state. If we set up the expectation that the data updates automatically, we ought to notify the user about being disconnected and ongoing reconnection attempts.

When detecting a disconnection, it’s useful to store data in memory (or even better, localStorage) so that it can be sent later. This is specially important in light of the introduction of ServiceWorker, which enables JavaScript web applications to run in the background. If your application is not open, you can still attempt to sync user data in the background.

Consider timeouts and errors when sending data and retry on behalf of the user. If a connection is re-established, attempt to send the data again. In the case of a persistent failure, communicate it to the user.

Certain errors should be handled carefully. For example, an unexpected 403 could mean the user’s session has been invalidated. In such cases, you have the opportunity to prompt the user to resume it by showing a login screen.

It’s also important to make sure the user doesn’t inadvertently interrupt the data flow. This can happen under two situations. The first and most obvious one is closing the browser or tab, which you can attempt to prevent with beforeunload handlers.


The beforeunload browser warning

The other (and less obvious) one is capturing page transitions before they happen, like clicking links that trigger a new page load. This gives you a chance to display your own modals.

 5. Don’t break history, enhance it

tl;DR: Without the browser managing URLs and history for us, new challenges emerge. Make sure not to break expectations related to scrolling. Keep your own caches for fast feedback.
Form submissions aside, if we were to design any modern web application with only hyperlinks, we’d end up with fully functional back/forward navigation.

Consider, for example, the typical “infinite pagination scenario”. The typical way it’s implemented involves capturing the click with JavaScript, requesting some data / HTML, injecting it. Making the history.pushState or replaceState call is an optional step, unfortunately not taken by many.

And this is why I use the word “break”. With the simpler model the web proposed initially, this situation was not in the picture. Every state transition relied on a URL change.

The flip side of this is that new opportunities emerge for enhancing history now that we can control it with JavaScript.

One such opportunity is what Daniel Pipius dubbed Fast Back:

Back should be quick; users don’t expect data to have changed much.

This is akin to considering the back button an application-level button and applying principle 2: act immediately on user input. The key is that you can now decide how to cache the previous page and render it instantly. You can then apply principle 3 and then inform the user of new data changes that happened to that page.

There’s still a few cases where you won’t be in control of the caching behavior. For example, if you render a page, then navigate to a third party website, and the user clicks back. Applications that render HTML on the server and then modify it on the client are at particular risk of this subtle bug:


Pressing back incorrectly loads the initial HTML from the pageload

Another way of breaking navigation is by ignoring scrolling memory. Once again, pages that don’t rely on JS and manual history management most likely won’t have an issue with this. But dynamic ones usually do. I tested the two most popular JavaScript-driven newsfeeds of the web: Twitter and Facebook. Both exhibited scrolling amnesia.


Infinite pagination is usually susceptible to scrolling amnesia

Finally, be aware of state changes that are relevant only while navigating history. Consider this example of toggling the display of comment subtrees.


The toggling of comments should be preserved when navigating history

If the page was re-rendered by following a link within the application, the expectation of the user might be that all comments appear uncollapsed. The state was volatile and only associated with the entry in the history stack.

 6. Push code updates

tl;DR: Pushing data without pushing code is insufficient. If your data updates automatically, so should your code. Avoid API errors and improve performance. Use stateless DOM for side-effect free repainting.
Making your application react to code changes is crucially important.

First of all, it reduces the surface for possible errors and increases reliability. If you make a breaking change to your backend APIs, then clients’ code must be updated. They might otherwise not be able to understand new data, or they may send data in an incompatible format.

Another equally important reason has to do with the implementation of principle #3. If your UI is self-updating, there’s little reason for users to trigger a page refresh.

Keep in mind that in a traditional website, a page refresh accomplishes two things: reload the data and reload the code. Setting up a mechanism to push data without one to push code is not enough, especially in a world where a single tab (session) might stay open for a very long time.

If a server push channel is in place, a notification can be emitted to clients when new code is available. In the absence of that, a version number can be appended as a header to outgoing HTTP requests. The server can then compare it to its latest known version, opt to handle request or not, and advice the client.

After this, some web applications opt to refresh the page on behalf of the user when deemed appropriate. For example, if the page is not visible and no form inputs are filled out.

A better approach is to perform hot code reloading. This means that there would be no need to perform a full page refresh. Instead, certain modules can be swapped on the fly and their code re-executed.

It’s certainly hard to make hot code reloading work for many existing codebases. It’s worth discussing then a type of architecture that elegantly separates behavior (code) from data (state). Such a separation would allow us to make a lot of different patches very efficient.

Consider for example a module in your application that sets up an event bus (e.g: socket.io). When events are received, the state of a certain component is populated and it renders to the DOM. Then you modify the behavior of that component, for example, so that it produces different DOM markup for existing and new state.

The ideal scenario is that we’re able to update the code on a per-module basis. It wouldn’t make sense to restart the socket connection, for example, if we can get away with just updating the modified component’s code. Our ideal architecture for hot-code pushing is thus modular.

But the next challenge is that modules should be able to be re-evaluated without introducing undesirable side effects. This is where an architecture like the one proposed by React comes particularly handy. If a component code is updated, its logic can be trivially re-executed and the DOM efficiently updates. An exploration of this concept by Dan Abramov can be found here.

In essence, the idea that you render to the DOM (or paint it) is what significantly helps with hot code swapping. If state was kept in the DOM, or event listeners where set up manually by your application, updating code would become a much more complicated task.

 7. Predict behavior

tl;DR: Negative latency.
A rich JavaScript application can have mechanisms in place for predicting the eventual user input.

The most common application of this idea is to preemptively request data from the server before an action is consummated. Starting to fetch data when you hover a hyperlink so that it’s ready when it’s clicked is a straightforward example.

A slightly more advanced method is to monitor mouse movement and analyze its trajectory to detect “collisions” with actionable elements like buttons. A jQuery example:

jQuery plugin that predicts the mouse trajectory
jQuery plugin that predicts the mouse trajectory

 Conclusion

The web remains one of the most versatile mediums for the transmission of information. As we continue to add more dynamism to our pages, we must ensure that we retain some of its great historical benefits while we incorporate new ones.

Pages interconnected by hyperlinks are a great building block for any type of application. Progressive loading of code, style and markup as the user navigates through them will ensure great performance without sacrificing interactivity.

New unique opportunities have been enabled by JavaScript that, once universally adopted, will ensure the best possible user experience for the broadest and freest platform in existence.

http://rauchg.com/2014/7-principles-of-rich-web-applications/


